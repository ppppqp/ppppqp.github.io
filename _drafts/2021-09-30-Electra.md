---
title:  "[NLP] Electra"
date:   2021-10-5 21:29:23 +0800
categories: Note ML
tags: NLP
---

> A brief intro to Electra paper and the context.


In this post I will talk a little bit about Electra(Efficiently Learning an Encoder that Classifies Token Replacements Accurately). I have to say you researchers are really good at naming stuff. 

## Context

## Problem

## ðŸ’¡ Electra's Idea


## Electra's Approach

### Replaced Token Detection

As we all know, bert uses MLM (Masked language model) as one of the training tasks. In MLM, a portion of words is masked by `[MASK]` token before fed into the model as input. The model is then trained to predict the masked word. Bert uses this approach because it can deeply use the bi-directional context.

In Electra, we choose to use a slightly different approaches called RTD(Replaced token detection). In RTD, the model is fed not with the `[MASK]` token but with a replaced word. It is then trained to detect which word is replaced. Specifically, the researchers train two neural networks, a generator $G$ and a discriminator $D$. Both consists of encoders(e.g. Transformers) that maps a sequence on input tokens $x=[x_1, \cdots,x_n]$ into "a sequence of contextualized vector $h(x) = [h_1, \cdots, h_n]$". For each token $x_i$, $h_i$ is the corresponding representation in the context. Then, for a given position $t$ where $x_t =$ `[MASK]`, the generator outputs its probability distribution of token(literally performing a MLM task) and replace `[MASK]` token with the highest probable token. The generated sentence with replaced token is fed into the discriminator. The discriminator predicts for each position, whether the token on that position is replaced or not.


#### Input to $G$

Original input is a sentence reprensented by a token vector with length $n$: 
$$x = [x_1, \cdots, x_n]$$

Then, MLM first select a random set (uniform distribution) of $k$ positions to mask out. The selected positions can be represented by 
$$m = [m_1, m_2,\cdots, m_k]$$ where $m_i \in [1, n], m_i \sim \text{unif}\{1,n\}$

All tokens in the corresponding position is replaced with `[MASK]` token.

$$x_{m_i} := \text{[MASK]}$$ 

The modified vector $x^{\text{masked}}$ is then passed to $G$.

#### Output of $G$ 

The output of the generator is a softmax function, representing the probability distribution of tokens at position $t$ given the context.
$$p_G(x_t | \mathbb{x}) = \dfrac{\exp(e(x_t)^Th_G(\mathbb{x})_t)}{\underset{x^\prime}{\sum}\exp(e(x^\prime)^Th_G(\mathbb{x})_t)}$$

where 

Notation | Description
|---|---|
|$e(x_t)$|The token embedding for token $x_t$|
|$h_G(x)_t$ | The sentence embedding for $x$ respect to $x_t$|
|$p_G(x_tï½œx)$| Probability of token $x_t$ given the context $x$|

The "corrupted" sequence $x^{\text{corrupted}}$, where the `[MASK]` tokens are replaced by $G$'s predictions is then passed to $D$. 

The output of $G$ is trained by maximum likelihood loss(corss entropy loss).

$$\mathcal{L}_{\text{MLM}}(x, \theta_G) = \mathbb{E}(\underset{i\in m}{\sum} - \log p_G(x_i|x^{\text{masked}}))$$

Intuitivly speaking, the cross entropy loss should compare two distributions, $p_{\text{data}}$ and $p_{\text{model}}$ with the following form:
$$\mathcal{L}(\theta) = -\mathbb{E}_{x,y \sim \hat p_{data}}\log{p_{model}(y|x)} $$

It is not hard to make the linkage between $\log p_G(x_i|x^{\text{masked}})$  and   $\log{p_{model}(y|x)}$. But where is the probability distribution $\mathbb{p_{x,y \sim \hat p_{data}}}$ goes? It actually lies in $\underset{i\in m}{\sum}$ because it is actually a set of dirac delta functions:

$$\mathbb{p_{x,y \sim \hat p_{data}}(x_i)} = \left\{
    \begin{aligned}
    1 \ & i\in m\\
    0 \ &i \notin m
    \end{aligned}
    \right.$$ 


For more explanation (only in chinese, sorry), you can visit [here](https://panqiping.gitbook.io/deep-learning-bible-note/shen-du-qian-kui-wang-luo/gradient-learning/loss-function)



#### Output of $D$

The output of a discriminator is represented by the following equation:

$$D(x,t) = \text{sigmoid}(w^T h_D(x)_t))$$

where 

Notation | Description
|---|---|
|$w$|Weight for the output layer|
|$h_D(x)_t$ | The sentence embedding for $x$ respect to $x_t$|
|$D(x,t)$| Probability of token $x_t$ is replaced|


The loss function for $D$ is also a maximum likelihood function.

$$\mathcal{L}_{\text{Disc}}(x, \theta_D) = \mathbb{E}\Big(\underset{t= 1}{\overset{n}{\sum}} - \mathbb{I}(x_t^{\text{corrupt}} = x_t)\log D(x_i|x^{\text{masked}})- \mathbb{I}(x_t^{\text{corrupt}} \neq x_t)\log(1- D(x_i|x^{\text{masked}}))\Big)$$

It is the same idea as above. The only difference is we use an indicator function $\mathbb{I}$ to represent $\mathbb{p_{x,y \sim \hat p_{data}}}$(forgive me for not able to type mathbb{1}. Hard to config latex).


### How to Use Electra
