

ğŸŒ°LangChain In a NutShell
LangChainçš„èƒŒæ™¯ï¼ŒåŸºæœ¬æ¦‚å¿µ
What is LangChain?
LangChainæ˜¯ä¸€ä¸ªçš„æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©å¼€å‘äººå‘˜ä½¿ç”¨è¯­è¨€æ¨¡å‹æ„å»ºç«¯åˆ°ç«¯çš„åº”ç”¨ç¨‹åºã€‚å®ƒæä¾›äº†ä¸€å¥—å·¥å…·ã€ç»„ä»¶å’Œæ¥å£ï¼Œå¯ç®€åŒ–åˆ›å»ºç”±LLM å’ŒèŠå¤©æ¨¡å‹æä¾›æ”¯æŒçš„åº”ç”¨ç¨‹åºçš„è¿‡ç¨‹ã€‚LangChain å¯ä»¥è½»æ¾ç®¡ç†ä¸è¯­è¨€æ¨¡å‹çš„äº¤äº’ï¼Œå°†å¤šä¸ªç»„ä»¶é“¾æ¥(chain)åœ¨ä¸€èµ·ï¼Œå¹¶é›†æˆé¢å¤–çš„èµ„æºï¼Œä¾‹å¦‚ API å’Œæ•°æ®åº“ã€‚
å®˜ç½‘ï¼š
https://www.langchain.com/
GitHub:
https://github.com/langchain-ai/langchain
å·²ç»æœ‰äº†ChatGPTï¼Œä¸ºä»€ä¹ˆè¦å¼€å‘LangChain?
æ¢å¥è¯è¯´ï¼Œæœ‰ä»€ä¹ˆä»»åŠ¡æ˜¯ChatGPTåšä¸äº†ä½†æ˜¯LangChainèƒ½åšçš„ï¼Ÿ
æ•°æ®
æˆ‘ä»¬æƒ³é€šè¿‡LLMå¾—åˆ°å…³äºæˆ‘ä»¬è‡ªå·±æ•°æ®çš„ä¸€äº›æ´å¯Ÿï¼Œä½†æ•°æ®é‡å¾ˆå¤§ï¼Œè¶…è¿‡äº†max input tokençš„é™åˆ¶æˆ–è€…ä¸Šä¸‹æ–‡è®°å¿†çš„é™åˆ¶ï¼Œæ— æ³•ç›´æ¥å–‚ç»™LLMã€‚å¦‚æœæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨LLMçš„èƒ½åŠ›åˆ™éœ€è¦å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†å’Œå­˜å‚¨ï¼Œæ¯”å¦‚æ–‡æœ¬åˆ†æ®µï¼Œæ•°æ®åº“å­˜å‚¨ç­‰ç­‰ã€‚LangChainæä¾›äº†è¿™ä¸€ç³»åˆ—èƒ½åŠ›ã€‚
ä¸å®¢è§‚ä¸–ç•Œäº¤äº’
å‡è®¾æˆ‘ä»¬æƒ³å®ç°ä¸€ä¸ªå¤©æ°”ä¸»æ’­chatbotï¼Œå¯ä»¥é€šè¿‡å¯¹è¯è·çŸ¥å…³äºå½“å¤©å¤©æ°”çš„ä¿¡æ¯ã€‚é€šè¿‡ChatGPTæ˜¯å®ç°ä¸äº†çš„ï¼Œå› ä¸º
1. ChatGPTçš„è®­ç»ƒæ•°æ®æ˜¯é™ˆæ—§çš„ï¼Œå…¶ä¸­ä¸åŒ…å«æœ€æ–°çš„å¤©æ°”ä¿¡æ¯
2. ChatGPTæ— æ³•é€šè¿‡è®¿é—®ç½‘ç«™çš„æ–¹å¼è·å–æœ€æ–°çš„å¤©æ°”ä¿¡æ¯ã€‚
LangChainä¸ºLLMèµ‹èƒ½å»å®ç°ä¸€äº›å¤–éƒ¨ä»»åŠ¡ï¼Œæ¯”å¦‚è¿è¡Œä»£ç æˆ–è€…è®¿é—®ç½‘ç«™ï¼Œä»¥æ­¤æ¥æ„å»ºæœ‰è¿™äº›éœ€æ±‚çš„chatbotã€‚
æŠ½è±¡
åœ¨å¼€å‘æ—¶ä¼šé‡åˆ°çš„é—®é¢˜æ˜¯æ¯ä¸ªLLM hostçš„APIå‚æ•°éƒ½ä¸ä¸€è‡´ï¼Œå¾€å¾€åˆ‡æ¢ä¸€ä¸ªæ¨¡å‹å°±éœ€è¦é‡æ–°å¼€å‘ä¸€éæ¥å£ã€‚LangChainé€šè¿‡æŠ½è±¡æˆç±»å®ç°äº†å„ä¸ªLLMçš„å®ç°ä¹‹é—´çš„åˆ‡æ¢ï¼ˆç±»ä¼¼huggingfaceä¸­åˆ‡æ¢ä¸åŒæ¨¡å‹ï¼Œåªéœ€è¦æ— ç—›plug&playï¼‰

Concepts
Langchainæ˜¯å¦‚ä½•è®¾è®¡æŠ½è±¡çš„ï¼Ÿ
LLMï¼šå¤§è¯­è¨€æ¨¡å‹
LangChainä¸­å°†LLMæŠ½è±¡æˆä¸€ä¸ªç±»ï¼Œæ¥é€‚é…å„ç§å„æ ·çš„LLMå®ç°(OpenAI ChatGPT, Google PaLM, etc). å¯ä»¥åœ¨LangChainä¸­ç›´æ¥ç”¨æœ€æœ´ç´ çš„æ–¹å¼ä¸LLMå¯¹è¯ï¼Œè¿‡ç¨‹å°±å’Œè®©ä¸€ä¸ªLM autoregressiveåœ°ç”Ÿæˆnext token predictionä¸€æ ·ã€‚
import { OpenAI } from "langchain/llms/openai";

const llm = new OpenAI({
  temperature: 0.9,
});

const result = await llm.predict("What would be a good company name for a company that makes colorful socks?");
// "Feetful of Fun"
ğŸ§©Schema
LangChainä¸­å¯¹Messageçš„æŠ½è±¡ï¼Œå®šä¹‰äº†ä¸‰ç§Message
- ğŸ‘¶ HumanMessageï¼šäººç±»çš„input
- ğŸ–²ï¸ SystemMessageï¼šä½œä¸ºLLMçš„instructionçš„input
- ğŸ¤– AIMessageï¼šLLMçš„output
import { ChatOpenAI } from "langchain/chat_models/openai";
import { HumanMessage, ChatMessage, SystemMessage } from "langchain/schema";
const chat = new ChatOpenAI({
  temperature: 0
});
const result = await chat.predictMessages([
  new SystemMessage("You are a helpful assistant that translates English to French."),
  new HumanMessage("Translate this sentence from English to French. I love programming.")
]);

/*
  AIMessage {
    content: "J'adore la programmation."
  }
*/
 predict(m) ç­‰ä»·äº predictMessages([HumanMessage(m)])

ğŸ•¸ï¸Prompt Templates
å°†ç”¨æˆ·çš„inputåµŒå…¥ä¸€ä¸ªå›ºå®šçš„promptæ¨¡ç‰ˆä¸­å½¢æˆpromptï¼Œç»§è€Œä¼ é€’ç»™è¯­è¨€æ¨¡å‹ã€‚Prompt Template æœ‰åŠ©äºå°†ç”¨æˆ·è¾“å…¥å’Œå…¶ä»–åŠ¨æ€ä¿¡æ¯è½¬æ¢ä¸ºé€‚åˆè¯­è¨€æ¨¡å‹çš„æ ¼å¼ã€‚
What does it remind you of? Yes, 'template ${literal}' !
import { PromptTemplate } from "langchain/prompts";

const prompt = PromptTemplate.fromTemplate("What is a good name for a company that makes {product}?");

const formattedPrompt = await prompt.format({
  product: "colorful socks"
});
â›“ï¸Chain
Chainå°±æ˜¯ä¸¤ä¸ªåŠ¨ä½œçš„é›†åˆï¼Œå³å°†inputå…ˆä¼ å…¥PromptTemplateï¼Œç„¶åå†ä¼ å…¥LLMï¼Œå¾—åˆ°è¾“å‡ºã€‚
// Chain = LLM(Prompt Templates(user input))

import { OpenAI } from "langchain/llms/openai";
import { LLMChain } from "langchain/chains";
import { PromptTemplate } from "langchain/prompts";

const llm = new OpenAI({});
const prompt = PromptTemplate.fromTemplate("What is a good name for a company that makes {product}?");

const chain = new LLMChain({
  llm,
  prompt
});

// Run is a convenience method for chains with prompts that require one input and one output.
const result = await chain.run("colorful socks");
å¤šä¸ªå­Chainå¯ä»¥è¢«é“¾æ¥æˆä¸€ä¸ªå¤§Chain (é€šè¿‡SimpleSequentialChain è¿™ä¸ªç±»ï¼‰ã€‚Chainå¸®åŠ©æˆ‘ä»¬å°†ä¸€äº›å°çš„ç»„ä»¶ç»„åˆèµ·æ¥å®Œæˆæ›´å¤æ‚çš„åŠŸèƒ½ã€‚
æœ‰ç‚¹åƒUnixä¸­çš„pipeline
This content is only supported in a Feishu Docs
Output Parsers
Output Parsers è´Ÿè´£å°†è¯­è¨€æ¨¡å‹å“åº”æ„å»ºä¸ºæ›´æœ‰ç”¨çš„æ ¼å¼ã€‚å®ƒä»¬å®ç°äº†ä¸¤ç§ä¸»è¦æ–¹æ³•ï¼šä¸€ç§ç”¨äºæä¾›æ ¼å¼åŒ–æŒ‡ä»¤ï¼Œå¦ä¸€ç§ç”¨äºå°†è¯­è¨€æ¨¡å‹çš„å“åº”è§£æä¸ºç»“æ„åŒ–æ ¼å¼ã€‚è¿™ä½¿å¾—åœ¨æ‚¨çš„åº”ç”¨ç¨‹åºä¸­å¤„ç†è¾“å‡ºæ•°æ®å˜å¾—æ›´åŠ å®¹æ˜“ã€‚

ğŸ±Embeddings & Vector Stores
EmbeddingsæŒ‡æ–‡æœ¬åœ¨å‘é‡ç©ºé—´çš„æŠ•å½±ã€‚LangChainå¯¹embeddingä¹Ÿåšäº†æŠ½è±¡ï¼Œæ‰€ä»¥å¯ä»¥è‡ªç”±åœ°ä½¿ç”¨ä¸åŒçš„embeddingå®ç°ã€‚
import { OpenAIEmbeddings } from "langchain/embeddings/openai";

/* Create instance */
const embeddings = new OpenAIEmbeddings();

/* Embed queries */
const res = await embeddings.embedQuery("Hello world");
/*
[
   -0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,
    0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,
    0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,
   0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,
   -0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,
  -0.0019203906,   0.012161949,  -0.019194454,   0.030373365, -0.031028723,
   0.0036170771,  -0.007813894, -0.0060778237,  -0.017820721, 0.0048647798,
   -0.015640393,   0.001373733,  -0.015552171,   0.019534737, -0.016169721,
    0.007316074,   0.008273906,   0.011418369,   -0.01390117, -0.033347685,
    0.011248227,  0.0042503807,  -0.012792102, -0.0014595914,  0.028356876,
    0.025407761, 0.00076445413,  -0.016308354,   0.017455231, -0.016396577,
    0.008557475,   -0.03312083,   0.031104341,   0.032389853,  -0.02132437,
    0.003324056,  0.0055610985, -0.0078012915,   0.006090427, 0.0062038545,
      0.0169133,  0.0036391325,  0.0076815626,  -0.018841568,  0.026037913,
    0.024550753,  0.0055264398, -0.0015824712, -0.0047765584,  0.018425668,
   0.0030656934, -0.0113742575, -0.0020322427,   0.005069579, 0.0022701253,
    0.036095154,  -0.027449455,  -0.008475555,   0.015388331,  0.018917186,
   0.0018999106,  -0.003349262,   0.020895867,  -0.014480911, -0.025042271,
    0.012546342,   0.013850759,  0.0069253794,   0.008588983, -0.015199285,
  -0.0029585673,  -0.008759124,   0.016749462,   0.004111747,  -0.04804285,
  ... 1436 more items
]
*/
ä½¿ç”¨Embeddingçš„å¥½å¤„æ˜¯ï¼Œå¯ä»¥è‡ªç”±åœ°è¿›è¡Œå‘é‡çš„æ•°å€¼æ“ä½œï¼Œæ¯”å¦‚å­˜å‚¨å’Œé€šè¿‡ç›¸ä¼¼åº¦æŸ¥è¯¢ã€‚ä¸€ä¸ªå¸¸è§çš„pipelineæ˜¯
This content is only supported in a Feishu Docs
LangChainæä¾›çš„å­˜å‚¨æ–¹å¼
  - In Memoryï¼šå­˜åœ¨å†…å­˜ä¸­
  - AnalyticDB
  - Chroma
  - Elasticsearch
  - ...
ç›¸ä¼¼åº¦æŸ¥è¯¢çš„metric
  - Cosine
  - Intersection
  - Motyka
  - ...

ğŸ§ Memory
å¯¹äºä¸€ä¸ªå¯¹è¯å¼AI(conversational AI)æ¥è¯´ä¸å¯ç¼ºå°‘çš„éƒ¨åˆ†ï¼Œä½†ä¾‹å¦‚OpenAIçš„APIæ˜¯æ— çŠ¶æ€çš„ï¼Œæ‰€ä»¥è®°å¿†ä¸Šä¸‹æ–‡èƒ½åŠ›æˆä¸ºä¸€ä¸ªé€šç”¨çš„éœ€æ±‚ã€‚LangChainæä¾›äº†ä»¥ä¸‹æŠ½è±¡ï¼š
[Image]
ChatMessageHistory
ä¸€ä¸ªåº•å±‚çš„ç±»ï¼Œå¯ä»¥çœ‹åˆ°å°±æ˜¯è®°å½•äº†ä¸€ç³»åˆ—ç”¨æˆ·å’ŒLLMçš„chatMessage
LangChain ä¸»è¦é€šè¿‡èŠå¤©ç•Œé¢ä¸è¯­è¨€æ¨¡å‹è¿›è¡Œäº¤äº’ã€‚ChatMessageHistory ç±»è´Ÿè´£è®°ä½æ‰€æœ‰ä»¥å‰çš„èŠå¤©äº¤äº’æ•°æ®ï¼Œç„¶åå¯ä»¥å°†è¿™äº›äº¤äº’æ•°æ®ä¼ é€’å›æ¨¡å‹ã€æ±‡æ€»æˆ–ä»¥å…¶ä»–æ–¹å¼ç»„åˆã€‚è¿™æœ‰åŠ©äºç»´æŠ¤ä¸Šä¸‹æ–‡å¹¶æé«˜æ¨¡å‹å¯¹å¯¹è¯çš„ç†è§£ã€‚
import { ChatMessageHistory } from "langchain/memory";

const history = new ChatMessageHistory();
await history.addUserMessage("Hi!");
await history.addAIChatMessage("What's up?");
const messages = await history.getMessages();
console.log(messages);

/*
  [
    HumanMessage {
      content: 'Hi!',
    },
    AIMessage {
      content: "What's up?",
    }
  ]
*/
BufferMemory
ä¸€ä¸ªåŸºç¡€çš„ç±»ï¼Œåœ¨ChatMessageHistoryä¸Šæ„å»ºï¼Œç”¨æ¥ä¸ºChainå®ç°è®°å¿†åŠŸèƒ½ã€‚
import { OpenAI } from "langchain/llms/openai";
import { BufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";

const model = new OpenAI({});
const memory = new BufferMemory(); 
// This chain is preconfigured with a default prompt
const chain = new ConversationChain({ llm: model, memory: memory });
//åœ¨chainä¸­ä½¿ç”¨BufferMemory
const res1 = await chain.call({ input: "Hi! I'm Jim." });
console.log({ res1 }); // {response: " Hi Jim! It's nice to meet you. My name is AI. What would you like to talk about?"}
const res2 = await chain.call({ input: "What's my name?" });
console.log({ res2 }); // {response: ' You said your name is Jim. Is there anything else you would like to talk about?'}
// å¯è§LLMè®°ä½äº†ä¹‹å‰çš„å¯¹è¯å†…å®¹
é™¤æ­¤ä¹‹å¤–ï¼ŒLangChainè¿˜å®ç°äº†å¾ˆå¤šåˆ«çš„Memoryç±»å‹
- BufferWindowMemoryç­‰ç­‰ï¼ˆè§ä¸Šå›¾ï¼‰: åªä¿å­˜æœ€è¿‘çš„kä¸ªinteractions
- ConversationSummaryMemory ï¼ˆè§ä¸Šå›¾ï¼‰: åœ¨å¯¹è¯çš„è¿‡ç¨‹ä¸­é€æ­¥åˆ›å»ºä¸€ä¸ªå¯¹è¯çš„summaryã€‚å¯ä»¥åº”ç”¨äºé•¿å¯¹è¯ï¼Œæ¥ä¿è¯å…³äºå¯¹è¯çš„è®°å¿†ä¸ä¼šå ç”¨å¤ªå¤šçš„tokenã€‚
ğŸ‘¶: Hi! I'm Jim.
ğŸ¤–: ...
ğŸ‘¶: What's my name?
ğŸ¤–: ...

memory: {
    chat_history: 'Jim introduces himself to the AI and the AI greets him and offers assistance. The AI addresses Jim by name and asks how it can assist him.'
}
- EntityMemoryï¼šä¿å­˜å…³äºå®ä½“(entity)çš„è®°å¿†ã€‚
æ¯”å¦‚å¦‚ä¸‹å¯¹è¯ï¼š
ğŸ‘¶: Hi! I'm Jim.
ğŸ¤–: ...
ğŸ‘¶: I work in sales. What about you?
ğŸ¤–: ...
ğŸ‘¶: My office is the Utica branch of Dunder Mifflin. What about you?
ğŸ¤–: ...
memory:{
    entities: {
        Jim: 'Jim is a human named Jim who works in sales.',
        Utica: 'Utica is the location of the branch of Dunder Mifflin where Jim works',
        'Dunder Mifflin': Dunder Mifflin has a branch in Utica
    }
}
- VectorStoreRetrieverMemoryï¼š å°†memoryå­˜åœ¨VectorDBä¸­ï¼Œå¹¶åœ¨è°ƒç”¨çš„æ—¶å€™queryå‰Kä¸ªæœ€æœ‰å…³çš„å¯¹è¯ã€‚
ğŸ› ï¸Tools
LLMç”¨æ¥ä¸ç°å®ä¸–ç•Œäº¤äº’çš„å·¥å…·ã€‚æ¯”å¦‚chatGPT plugin, AWS Lambda,  Web browser, Bing, Youtube, Gmailç­‰ç­‰ã€‚è¿™äº›éƒ½å¯ä»¥ç»Ÿä¸€é€šè¿‡Toolçš„æŠ½è±¡æ¥åœ¨LangChainä¸­è¢«ä½¿ç”¨ã€‚
interface Tool {
  call(arg: string): Promise<string>;
  name: string;
  description: string;
}
ğŸ•µï¸â€â™€ï¸Agent
Agent æ˜¯åœ¨ LangChain ä¸­æ¨åŠ¨å†³ç­–åˆ¶å®šçš„å®ä½“ã€‚ä»–å¯ä»¥æ ¹æ®ç”¨æˆ·è¾“å…¥å†³å®šè°ƒç”¨å“ªä¸ªå·¥å…·æ¥å®Œæˆç‰¹å®šä»»åŠ¡å’Œä¸€ç³»åˆ—LLMçš„è°ƒç”¨ã€‚ç®€å•æ¥è¯´ï¼ŒAgentå¯ä»¥ç†è§£ä¸ºâ€œå€ŸåŠ©å·¥å…·å®Œæˆç‰¹å®šä»»åŠ¡çš„LLMåº”ç”¨â€ï¼Œæ¯”å¦‚Creative Assistantå°±å¯ä»¥çœ‹ä½œä¸€ä¸ªAgentï¼šå®ƒé€šè¿‡ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ç”ŸæˆæŒ‡ä»¤æ¥æœç´¢ç´ æåº“æˆ–è€…ç”Ÿæˆè„šæœ¬ï¼Œä»¥æ­¤æ»¡è¶³éœ€æ±‚ã€‚Agentæ˜¯LangChainçƒ­åº¦æ¯”è¾ƒé«˜çš„featureã€‚å½“ç„¶ä¹Ÿæœ‰åˆ«çš„Agent APIï¼Œæ¯”å¦‚AgentGPT: Autonomous AI in your browser ğŸ¤–ï¼Œä½†LangChainçš„ä¼˜åŠ¿åœ¨äºå¼ºå¤§çš„å·¥å…·èƒ½åŠ›å’ŒæŠ½è±¡ã€‚
Agent Types | ğŸ¦œï¸ğŸ”— Langchain

---
Landscape
LangChainä¹Ÿåœ¨ç§¯æåœ°åšä¸€äº›ç”Ÿæ€å»ºè®¾
[Image]
LangChain Expression Language
The quickest way to prototype the brains of your LLM application
https://blog.langchain.dev/langchain-expression-language/
Hightlight
ä½ä»£ç 
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate

model = ChatOpenAI()
prompt = ChatPromptTemplate.from_template("tell me a joke about {foo}")
chain = prompt | model

chain.invoke({"foo": "bears"})
>>> AIMessage(content="Why don't bears ever wear shoes?\n\nBecause they have bear feet!", additional_kwargs={}, example=False)
[Image]
Templates
LangChain Templates offers a collection of easily deployable reference architectures that anyone can use
https://blog.langchain.dev/langchain-templates/
[Image]
LangServe
https://blog.langchain.dev/introducing-langserve/
a hosted version of LangServe for one-click deployments of LangChain applications.
[Image]
highlight
- significantly easier to go from prototype to production and get a production ready API
How does it work
integrated with FastAPI and uses pydantic for data validation.
1. create our chain
"""A conversational retrieval chain."""

from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

vectorstore = FAISS.from_texts(
    ["cats like fish", "dogs like sticks"],
    embedding=OpenAIEmbeddings()
)
retriever = vectorstore.as_retriever()

model = ChatOpenAI()

chain = ConversationalRetrievalChain.from_llm(model, retriever)
2. pass that chain to add_routes
#!/usr/bin/env python
"""A server for the chain above."""

from fastapi import FastAPI
from langserve import add_routes

from my_package.chain import chain

app = FastAPI(title="Retrieval App")

add_routes(app, chain)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
That's it, this gets you a scalable web server

[Image]
- Built-in (optional) tracing to LangSmith, just add your API key as an environment variable
- Support for hosting multiple chains in the same server under separate paths
- All built with battle-tested open-source Python libraries like FastAPI, Pydantic, uvloop and asyncio.
LangSmith
How to build production level LLM app? å¦‚ä½•è¯æ˜/è¯„ä¼°llmåº”ç”¨æ˜¯å¦å·²ç»è¾¾åˆ°production levelï¼Ÿ
https://www.langchain.com/langsmith
LangSmithæ˜¯ä¸€ä¸ªall in oneçš„llm appè°ƒè¯•-éƒ¨ç½²-è¯„ä¼°å¹³å°ï¼ˆat beta stageï¼‰ã€‚å’ŒMLflow - A platform for the machine learning lifecycleå¾ˆç±»ä¼¼ã€‚
Tracingï¼šå¯ä»¥ä»¥æ—¥å¿—çš„å½¢å¼è®°å½•llmçš„æ¯ä¸€æ¬¡äº’åŠ¨ï¼Œæ€»ç»“åº”ç”¨çš„ä½¿ç”¨æƒ…å†µï¼Œè®°å½•llmå’Œå·¥å…·çš„è°ƒç”¨ã€‚è¿˜å¯ä»¥åœ¨playgroundé‡Œè°ƒè¯•promptå¹¶å®æ—¶æŸ¥çœ‹æ•ˆæœã€‚å¯¹äºagentï¼ŒLangChainæœ‰å¾ˆå¤šéšå¼çš„llmè°ƒç”¨ï¼Œé€šè¿‡tracingå¯ä»¥çœ‹åˆ°å…¶ä¸­çš„æ‰€æœ‰è°ƒç”¨ã€‚
[Image]

[Image]

Evaluation: llmä¸­æœ‰å¾ˆå¤šéš¾é¢˜ï¼Œæ¯”å¦‚è¾“å‡ºçš„ä¸ç¡®å®šæ€§ï¼Œå®‰å…¨ï¼ˆprompt injectionï¼‰ï¼Œå“åº”å»¶è¿Ÿç­‰ç­‰ã€‚LangSmithæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„è¯„ä¼°æ–¹æ¡ˆã€‚
åˆ›å»ºæ•°æ®é›†ï¼šä¸€ä¸ªè¾“å…¥å­—ç¬¦ä¸²æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ ä½œä¸ºå¯¹llmçš„ä¸€ä¸ªtest case
è¯„ä¼°æ–¹å¼(evaluator): LangSmithä¸­æä¾›äº†å¤šç§evaluator
- QA evaluatorï¼šæœ€å¸¸è§çš„ï¼ŒåŸºäºé—®ç­”å‡†ç¡®æ€§çš„è¯„ä¼°
- Criteria evaluator: æ¯”è¾ƒæœ‰åˆ›æ–°æ€§çš„ï¼ŒåŸºäºå‡†åˆ™çš„è¯„ä¼°ã€‚å¯ä»¥é€‰æ‹©çš„å‡†åˆ™æœ‰ï¼šcreativityï¼ˆæ˜¯å¦æœ‰åˆ›æ–°æ€§ï¼‰ï¼Œconcisenessï¼ˆæ˜¯å¦ç²¾ç»ƒï¼‰ï¼Œrelevanceï¼ˆæ˜¯å¦ç›¸å…³ï¼‰ï¼Œcoherenceï¼ˆæ˜¯å¦ä¸€è‡´ï¼‰ç­‰ç­‰ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ªcriteriaã€‚æœ¬è´¨ä¸Šä¹Ÿæ˜¯åŸºäºpromptçš„ "Is this response imaginative? Response Y if they are, N if they are not"
[Image]
[Image]


ğŸ–ï¸Get Hands Dirty: Building Your Own LLM App
æ„å»ºä¸€ä¸ªç®€å•çš„LangChainåº”ç”¨ï¼Œæ¢ç´¢å„ç§èƒ½åŠ›
LangChainçš„ä½¿ç”¨
Installation
ä»¥langchian.jsä¸ºä¾‹ï¼Œæˆ‘ä»¬æ¥åˆ›å»ºä¸€ä¸ªç®€å•çš„LLM åº”ç”¨ã€‚
åˆ›å»ºä¸€ä¸ªæ–°çš„node.jsé¡¹ç›®ï¼Œï¼ˆnode.js > 18)
mkdir langchainTest
cd langchainTest
npm init es6 -y
å®‰è£…langchian
npm install langchain
Basicï¼šLangChainåŸºç¡€èƒ½åŠ›
ä½¿ç”¨LangChainçš„æŠ½è±¡ç±»ï¼Œå¯ä»¥éšè—è°ƒç”¨OpenAI APIçš„å¤æ‚åº¦
import { OpenAI } from "langchain/llms/openai";
import "dotenv/config";

const llm = new OpenAI({
  openAIApiKey: process.env.OPENAI_API_KEY,
  temperature: 0.9, // Temperature is a parameter of OpenAI ChatGPT, GPT-3 and GPT-4 models that governs the randomness and thus the creativity of the responses. It is always a number between 0 and 1
});

const result = await llm.predict(
  "What would be a good company name for a company that makes colorful socks?"
);
console.log(result);
// Happy Sockery.
é€šè¿‡templateå®šåˆ¶å’Œç®¡ç†prompt
import { PromptTemplate } from "langchain/prompts";
import "dotenv/config";

const prompt = PromptTemplate.fromTemplate(
  "What is a good name for a company that makes {product}?"
);

const formattedPrompt = await prompt.format({
  product: "colorful socks",
});

console.log(formattedPrompt);
// What is a good name for a company that makes colorful socks?
LLM Chainï¼šå°†llmå’Œprompt templateç»“åˆåœ¨ä¸€èµ· 
ä¼¼æ›¾ç›¸è¯†çš„ä¸»é¢˜ - ref: è¯­è¨€æ¨¡å‹ä¸PromptæŠ€å·§ 
import 'dotenv/config';
import { OpenAI } from 'langchain/llms/openai';
import { LLMChain } from 'langchain/chains';
import { PromptTemplate } from 'langchain/prompts';

const llm = new OpenAI({
  openAIApiKey: process.env.OPENAI_API_KEY,
  maxTokens: 3072,
});

const prompt = PromptTemplate.fromTemplate(
  `
  ä½ æ˜¯Tiktokçˆ†æ¬¾è§†é¢‘å¹¿å‘Šè„šæœ¬å†™ä½œä¸“å®¶, è¯·ä½ ç”¨ä¸€ä¸‹æ­¥éª¤è¿›è¡Œåˆ›ä½œ, äº§å‡ºä¸€æ®µæ‹æ‘„è„šæœ¬

  åœ¨å†™ä½œTiktokæ‹æ‘„è„šæœ¬æ–¹é¢, ä½ ä¼šä»¥ä¸‹æŠ€èƒ½
    1 .å¼€å¤´å¼•å…¥hookå¸å¼•è§‚ä¼—è§‚çœ‹
    2. åœ¨è„šæœ¬ä¸­é—´éƒ¨åˆ†å°†å–ç‚¹èå…¥æ‹æ‘„ä¸»ä½“
    3. åœºæ™¯ä¸­çš„ä¸»è¦è¡Œä¸º(å¦‚ç¾½æ¯›çƒåœºä¸Šä¸»è¦è¡Œä¸ºä¸ºæ‰“ç¾½æ¯›çƒ)éœ€è¦ä¸äº§å“äº§ç”Ÿäº’åŠ¨
    4. è„šæœ¬ç»“å°¾é‡‡ç”¨äº’åŠ¨å¼è¯­è¨€ä¸è§‚ä¼—å»ºç«‹çº½å¸¦

  è¾“å…¥è§„åˆ™: æˆ‘ä¼šæŒ‰ä¸‹è¿°æ ¼å¼ç»™äºˆä½ ä¿¡æ¯
  æ‹æ‘„ä¸»é¢˜:
  æ‹æ‘„åœºæ™¯:
  æ‹æ‘„ä¸»è§’:
  äº§å“:
  ä¸»è¦å–ç‚¹:

  è¾“å‡ºè§„åˆ™: ç»“åˆæˆ‘ç»™ä½ è¾“å…¥çš„ä¿¡æ¯, ç»™ä¸ä½ çš„å®ä¾‹, ä»¥åŠä½ æŒæ¡çš„ç¼–å†™æŠ€å·§, äº§å‡ºå†…å®¹, è¯·ä¸¥æ ¼æŒ‰ç…§ä¸€ä¸‹æ ¼å¼è¾“å‡ºå†…å®¹, åªéœ€è¦æ ¼å¼æè¿°çš„å†…å®¹, å¦‚æœäº§å‡ºå…¶ä»–å†…å®¹åˆ™ä¸è¾“å‡º
  |åœºæ™¯|æ—¶é•¿|å†…å®¹|

  æ‹æ‘„ä¸»é¢˜: {title}
  æ‹æ‘„åœºæ™¯: {scene}
  æ‹æ‘„ä¸»è§’: {actor}
  äº§å“: {product}
  ä¸»è¦å–ç‚¹: {feature}
  `
);
const chain = new LLMChain({
  llm,
  prompt,
});

const result = await chain.call({
  title: 'æ‰“ç¾½æ¯›çƒéœ€è¦ä»€ä¹ˆ',
  scene: 'ç¾½æ¯›çƒåœº',
  actor: 'ç‹å°ç¾½',
  product: 'ç¾çš„xxxæ´—è¡£æœº',
  feature:
    'æ‹¥æœ‰é€Ÿæº¶é¢„æ··åŠŸèƒ½, å‡çš„æ´—è¡£æ¶²ä¼šæå‰åœ¨é¢„æ··ä»“å……åˆ†æ…æ‹Œ, æ··åˆæˆ3å€æµ“åº¦çš„ç²¾åæ¶², å…·æœ‰å¾ˆå¼ºçš„æ¸…æ´åŠ›',
});
console.log(result);
{
  text: '\n' +
    '  |åœºæ™¯1|10s|ç”»é¢æ¨å‡ºï¼Œç‹å°ç¾½åœ¨ç¾½æ¯›çƒåœºä¸Šé¢è¯´ï¼šâ€œæ¯æ¬¡æ‰“ç¾½æ¯›çƒï¼Œæˆ‘éƒ½æ‹…å¿ƒè¡£æœè„äº†å¯æ€ä¹ˆåŠï¼Ÿâ€|\n' +
    '  |åœºæ™¯2|5s|ç”»é¢åˆ‡æ¢ï¼Œç‹å°ç¾½åœ¨å®¶é‡Œï¼ŒæŠŠè¡£æœæ”¾è¿›ç¾çš„xxxæ´—è¡£æœºï¼Œå¹¶ä¸”æ‰“å¼€æ´—è¡£æœºï¼Œå¼€å§‹æ´—è¡£|\n' +
    '  |åœºæ™¯3|5s|ç”»é¢åˆ‡æ¢ï¼Œæ´—è¡£æœºç•Œé¢ï¼Œæ´—è¡£æœºæ­£åœ¨æé†’ç‹å°ç¾½é‡Šæ”¾é¢„æ··ä»“çš„æ´—è¡£æ¶²ï¼Œæ´—è¡£æ¶²æ··åˆæˆ3å€æµ“åº¦çš„ç²¾åæ¶²ï¼Œä»¥æ›´æœ‰æ•ˆçš„æ¸…æ´æ•ˆæœ|\n' +
    '  |åœºæ™¯4|5s|ç”»é¢åˆ‡æ¢ï¼Œç‹å°ç¾½æ‹¿å‡ºæ´—å¥½çš„è¡£æœï¼Œå¹¶ä¸”è¯´ï¼šâ€œç¾çš„xxxæ´—è¡£æœºï¼Œæ‹¥æœ‰é€Ÿæº¶é¢„æ··åŠŸèƒ½ï¼Œå‡çš„æ´—è¡£æ¶²ä¼šæå‰åœ¨é¢„æ··ä»“å……åˆ†æ…æ‹Œï¼Œæ··åˆæˆ3å€æµ“åº¦çš„ç²¾åæ¶²ï¼Œå…·æœ‰å¾ˆå¼ºçš„æ¸…æ´åŠ›ï¼Œè®©æˆ‘çœå¿ƒå’Œèˆ’å¿ƒï¼â€|\n' +
    '  |åœºæ™¯5|2s|ç”»é¢åˆ‡æ¢ï¼Œç‹å°ç¾½ç©¿ç€æ´—å¥½çš„è¡£æœï¼Œå‡†å¤‡ç»§ç»­æ‰“ç¾½æ¯›çƒï¼Œå¹¶ä¸”é¢å‘é•œå¤´è¯´ï¼šâ€œå¿«æ¥ç¾çš„xxxï¼Œè®©ä½ çš„ç”Ÿæ´»æ›´åŠ è½»æ¾ï¼â€|'
}
Agent: è°ƒç”¨å·¥å…·é“¾å®Œæˆä»»åŠ¡
å¯ä»¥æ ¹æ®ReActå¤„ç†ä¸€äº›å¤æ‚çš„ä»»åŠ¡, æ¯”å¦‚æˆ‘ä»¬è¯¢é—®ä¸€ä¸‹San fraciscoæ˜¨å¤©çš„æ°”æ¸©, å†è®©ä»–è¿›è¡Œä¸€äº›è®¡ç®—.
import { initializeAgentExecutorWithOptions } from "langchain/agents";
import { OpenAI } from "langchain/llms/openai";
import { SerpAPI } from "langchain/tools";
import { Calculator } from "langchain/tools/calculator";
import "dotenv/config";

const model = new OpenAI({
  temperature: 0,
  openAIApiKey: process.env.OPENAI_API_KEY,
});
const tools = [
  new SerpAPI(process.env.SERPAPI_API_KEY, {
    location: "Austin,Texas,United States",
    hl: "en",
    gl: "us",
  }),
  new Calculator(),
];

const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: "zero-shot-react-description",
  verbose: true,
});

const input =
  "What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?";

const result = await executor.call({
  input,
});

{ result: '1.1030087103157373' }
Agentå·¥ä½œæµç¨‹ç¤ºæ„
This content is only supported in a Feishu Docs
Memory: èŠå¤©è®°å½•çš„å­˜å‚¨
å¯ä»¥å‚¨å­˜èŠå¤©è®°å½•ï¼Œé€šè¿‡è¿™ä¸ªæ¨¡å¼å¯ä»¥è½»æ¾åœ°æ„é€ ä¸€ä¸ªé•¿æœŸå¯¹è¯ï¼Œä»¥æ­¤ä¸ºåŸºç¡€æ­å»ºä¸€ä¸ªèŠå¤©æœºå™¨äººã€‚
import { ConversationChain } from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
  MessagesPlaceholder,
} from "langchain/prompts";
import { BufferMemory } from "langchain/memory";
import "dotenv/config";

const chat = new ChatOpenAI({
  temperature: 0,
  openAIApiKey: process.env.OPENAI_API_KEY,
});

const chatPrompt = ChatPromptTemplate.fromPromptMessages([
  SystemMessagePromptTemplate.fromTemplate(
    "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know."
  ),
  new MessagesPlaceholder("history"),
  HumanMessagePromptTemplate.fromTemplate("{input}"),
]);

// Return the current conversation directly as messages and insert them into the MessagesPlaceholder in the above prompt.
const memory = new BufferMemory({
  returnMessages: true,
  memoryKey: "history",
});

const chain = new ConversationChain({
  memory,
  prompt: chatPrompt,
  llm: chat,
  verbose: true,
});

const res = await chain.call({
  input: "My name is Jim.",
});
console.log(await memory.chatHistory.getMessages());

// Console
[
  HumanMessage {
    lc_serializable: true,
    lc_kwargs: { content: 'My name is Jim.', additional_kwargs: {} },
    lc_namespace: [ 'langchain', 'schema' ],
    content: 'My name is Jim.',
    name: undefined,
    additional_kwargs: {}
  },
  AIMessage {
    lc_serializable: true,
    lc_kwargs: {
      content: "Hello Jim! It's nice to meet you. How can I assist you today?",
      additional_kwargs: {}
    },
    lc_namespace: [ 'langchain', 'schema' ],
    content: "Hello Jim! It's nice to meet you. How can I assist you today?",
    name: undefined,
    additional_kwargs: {}
  }
]

const res2 = await chain.call({
  input: 'What is my name?',
});

// Console
console.log(res2);
{
   response: 'Your name is Jim. You mentioned it at the beginning of our conversation. Is there anything specific you would like to know or discuss, Jim?'
}
Output Parser: å®šåˆ¶è¾“å‡ºæ ¼å¼
å¼ºåˆ¶LLMçš„è¾“å‡ºæ»¡è¶³ç‰¹å®šæ ¼å¼ã€‚ä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªåŠŸèƒ½ï¼š
- æ ¼å¼åŒ–è¾“å‡ºï¼šæ¯”å¦‚Creative Assistantå°±å­˜åœ¨éœ€è¦æ ¼å¼åŒ–æ•°æ®æ¥è®©GUIæ¶ˆè´¹çš„éœ€æ±‚ã€‚
- å¢å¼º/ä¼˜åŒ–æ¨¡å‹è¾“å‡ºç¨³å®šæ€§ï¼š
  - é”™è¯¯ä¿®æ­£ï¼ˆè¯­æ³•ï¼Œæ‹¼å†™ï¼‰
  - é£æ ¼ä¸€è‡´æ€§
  - å†…å®¹è¿‡æ»¤
import { z } from "zod";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { PromptTemplate } from "langchain/prompts";
import { LLMChain } from "langchain/chains";
import {
  StructuredOutputParser,
  OutputFixingParser,
} from "langchain/output_parsers";
import "dotenv/config";

const outputParser = StructuredOutputParser.fromZodSchema(
  z.array(
      z.object({
        fields: z.object({
          Name: z.string().describe("The name of the country"),
          Capital: z.string().describe("The country's capital"),
        }),
      })
    ).describe("An array of Airtable records, each representing a country"));

const chatModel = new ChatOpenAI({
  temperature: 0, // For best results with the output fixing parser
  openAIApiKey: process.env.OPENAI_API_KEY,
});

const outputFixingParser = OutputFixingParser.fromLLM(chatModel, outputParser);

// Don't forget to include formatting instructions in the prompt!
const prompt = new PromptTemplate({
  template: `Answer the user's question as best you can:\n{format_instructions}\n{query}`,
  inputVariables: ["query"],
  partialVariables: {
    format_instructions: outputFixingParser.getFormatInstructions(),
  },
});

const answerFormattingChain = new LLMChain({
  llm: chatModel,
  prompt,
  outputKey: "records", // For readability - otherwise the chain output will default to a property named "text"
  outputParser: outputFixingParser,
});

const result = await answerFormattingChain.call({
  query: "List 5 countries.",
});

console.log(JSON.stringify(result.records));

//console
[
  {
    "fields": {
      "Name": "United States",
      "Capital": "Washington, D.C."
    }
  },
  {
    "fields": {
      "Name": "Canada",
      "Capital": "Ottawa"
    }
  },
  {
    "fields": {
      "Name": "Germany",
      "Capital": "Berlin"
    }
  },
  {
    "fields": {
      "Name": "Japan",
      "Capital": "Tokyo"
    }
  },
  {
    "fields": {
      "Name": "Australia",
      "Capital": "Canberra"
    }
  }
]


API Chainï¼šè®¿é—®API
æ ¹æ®Docè®¿é—®API
[Image]
import { OpenAI } from "langchain/llms/openai";
import { APIChain } from "langchain/chains";
import * as fs from "fs";
import "dotenv/config";

const model = new OpenAI({
  modelName: "text-davinci-003",
  openAIApiKey: process.env.OPENAI_API_KEY,
});
const OPEN_METEO_DOCS = fs.readFileSync("OPEN_METEO_DOCS.txt", "utf8");
const chain = APIChain.fromLLMAndAPIDocs(model, OPEN_METEO_DOCS, {
  headers: {
    // These headers will be used for API requests made by the chain.
  },
});

const res = await chain.call({
  question:
    "What is the weather like right now in Munich, Germany in degrees Farenheit?",
});
console.log({ res });
/** console **/
{
  res: {
    output: ' The current weather in Munich, Germany is 70.7Â°F with a windspeed of 10.8 km/h.'
  }
}
Retrieval QAï¼šæ ¹æ®æ–‡ç« å›ç­”é—®é¢˜
RAG(Retrieval-Augmented Generation)ï¼šä»å¤§é‡æ–‡ç« ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯æ¥å¢å¼ºæ–‡æœ¬ç”Ÿæˆçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚RAGé€šå¸¸è¢«ç”¨æ¥å›ç­”å¤æ‚é—®é¢˜æˆ–è€…ç”Ÿæˆä¿¡æ¯å¯†é›†çš„å†…å®¹ã€‚
é€šè¿‡TextSplitterså’ŒVector Storeï¼ŒLangChainå¯ä»¥è½»æ¾å®ç°RAG
[Image]

import { OpenAI } from "langchain/llms/openai";
import { RetrievalQAChain, loadQAStuffChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";
import "dotenv/config";
import { PromptTemplate } from "langchain";

// Initialize the LLM to use to answer the question.
const model = new OpenAI({
  openAIApiKey: process.env.OPENAI_API_KEY,
});
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// Create a vector store from the documents.
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());

// Initialize a retriever wrapper around the vector store
const vectorStoreRetriever = vectorStore.asRetriever();

const promptTemplate = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

{context}

Question: {question}
Answer in Chinese:`;

const prompt = PromptTemplate.fromTemplate(promptTemplate);

// Create a chain that uses the OpenAI LLM and HNSWLib vector store.
const chain = new RetrievalQAChain({
  combineDocumentsChain: loadQAStuffChain(model, { prompt }),
  retriever: vectorStoreRetriever,
});
const res = await chain.call({
  query: "What did the president say about Justice Breyer?",
});
console.log({ res });

// Conosle
{
  res: {
    text: ' ä»–è¡¨æ‰¬äº†å²è’‚èŠ¬Â·å¸ƒé›·è€¶æœ€é«˜æ³•é™¢å¸æ³•å®˜çš„ç”Ÿæ¶¯ï¼Œè¯´ä»–æ˜¯ä¸ºå›½å®¶æœåŠ¡çš„äººï¼Œæ˜¯é©¬è¨è¯¸å¡å·çš„å†›äººã€å®ªæ³•å­¦è€…å’Œç¾å›½æœ€é«˜æ³•é™¢çš„é€€ä¼‘å¸æ³•å®˜ã€‚'
  }
}
Conversational Retrieval QA
æ ¹æ®æ–‡ç« èŠå¤©
import { ChatOpenAI } from "langchain/chat_models/openai";
import { ConversationalRetrievalQAChain } from "langchain/chains";
import { HNSWLib } from "langchain/vectorstores/hnswlib";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { BufferMemory } from "langchain/memory";
import * as fs from "fs";
import "dotenv/config";

/* Initialize the LLM to use to answer the question */
const model = new ChatOpenAI({
  openAIApiKey: process.env.OPENAI_API_KEY,
});
/* Load in the file we want to do question answering over */
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
/* Split the text into chunks */
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);
/* Create the vectorstore */
const vectorStore = await HNSWLib.fromDocuments(docs, new OpenAIEmbeddings());
/* Create the chain */
const chain = ConversationalRetrievalQAChain.fromLLM(
  model,
  vectorStore.asRetriever(),
  {
    memory: new BufferMemory({
      memoryKey: "chat_history", // Must be set to "chat_history"
    }),
  }
);
/* Ask it a question */
const question = "What did the president say about Justice Breyer?";
const res = await chain.call({ question });
console.log(res);
/* Ask it a follow up question */
const followUpRes = await chain.call({
  question: "Was that nice?",
});
console.log(followUpRes);

// *** Console ***
{
  text: 'The President honored Justice Stephen Breyer, recognizing him as an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. The President thanked Justice Breyer for his service.'
}

{
  text: "Yes, the President's acknowledgement of Justice Breyer's accomplishments and gratitude for his service was commendable."
}

SQL
è®¿é—®æ•°æ®åº“
import { DataSource } from "typeorm";
import { OpenAI } from "langchain/llms/openai";
import { SqlDatabase } from "langchain/sql_db";
import { SqlDatabaseChain } from "langchain/chains/sql_db";

const datasource = new DataSource({
  type: "sqlite",
  database: "Chinook.db",
});

const db = await SqlDatabase.fromDataSourceParams({
  appDataSource: datasource,
});

const chain = new SqlDatabaseChain({
  llm: new OpenAI({ temperature: 0 }),
  database: db,
});

const res = await chain.run("How many tracks are there?");
console.log(res);

// Console
There are 3503 tracks.
Summarization
æ–‡ç« æ€»ç»“Chain
import { OpenAI } from "langchain/llms/openai";
import { loadSummarizationChain } from "langchain/chains";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import * as fs from "fs";

// In this example, we use a `MapReduceDocumentsChain` specifically prompted to summarize a set of documents.
const text = fs.readFileSync("state_of_the_union.txt", "utf8");
const model = new OpenAI({ temperature: 0 });
const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });
const docs = await textSplitter.createDocuments([text]);

// This convenience function creates a document chain prompted to summarize a set of documents.
const chain = loadSummarizationChain(model, { type: "map_reduce" });
const res = await chain.call({
  input_documents: docs,
});
console.log({ res });

// Console
{
  res: {
    text: " President Biden's State of the Union address highlighted the resilience of the American people and nation, and the global support for Ukraine in the face of autocracy. He proposed a plan to create jobs, modernize infrastructure, and promote environmental justice in America, as well as reduce the cost of prescription drugs, invest in infrastructure and innovation, and combat inflation. He also called for increased competition in the market to reduce costs and protect consumers from exploitation, and for bipartisan support to pass a budget, reduce gun violence, and protect the right to vote. Finally, he announced an expansion of eligibility for benefits to include nine respiratory cancers, and urged Congress to fund ARPA-H, a project to make advances in treating cancer, Alzheimer's, and diabetes."
  }
}



æœªæ¥ï¼Ÿ
RAG -> More Context Support & File Input
- OpenAI Dev Dayç»™å‡ºçš„ä¸¤ä¸ªæ–°featureï¼Œæˆ–è®¸ä¼šæ‰“å‡»åˆ°LangChainæœ€é‡è¦çš„usecase ä¹‹ä¸€ï¼šRAG
  - 128,000 tokens of context
  - å¯ä»¥æ”¯æŒæ–‡ä»¶ä¸Šä¼ è¾“å…¥
- åˆ©ç”¨OpenAlçš„æ–°GPT-4V APIå‘å¸ƒä¸€ç³»åˆ—å¤šæ¨¡æ€RAG(æ£€ç´¢å¢å¼ºç”Ÿæˆ)æ¨¡æ¿å’Œæ•™ç¨‹
åœ¨å¤§å¤šæ•° RAG åº”ç”¨ä¸­ï¼Œå›¾åƒä¸­æ•è·çš„ä¿¡æ¯éƒ½ä¼šä¸¢å¤±ã€‚Langchainå‘å¸ƒäº†å‡ ä¸ªåˆ©ç”¨GPT-4V APIçš„æ¨¡ç‰ˆï¼Œè®©ç”¨æˆ·å¯ä»¥å®ç°å¤šæ¨¡æ€RAGï¼š
https://github.com/langchain-ai/langchain/blob/master/cookbook/Multi_modal_RAG.ipynb
[Image]

Agent -> Internet Access & Function calling & Code Interpreter
OpenAIå¢å¼ºäº†å‡½æ•°è°ƒç”¨çš„èƒ½åŠ›ï¼Œå¯ä»¥ä¸€æ¬¡è°ƒç”¨å¤šä¸ªå‡½æ•°ï¼Œç±»ä¼¼Agentã€‚
OpenAIå¯ä»¥ç›´æ¥ä½¿ç”¨äº’è”ç½‘ï¼Œä¹Ÿå¯ä»¥å…è®¸ä»£ç ï¼Œå†…ç½®äº†LangChainçš„toolingèƒ½åŠ›ã€‚
[Image]
Output Parser -> JSON mode
OpenAIä¼šç¡®ä¿JSONæ ¼å¼è¾“å‡ºï¼Œä¹Ÿå°±æ˜¯LangChainçš„Output Parserä¹Ÿå¯æ›¿ä»£äº†ã€‚
[Image]
 OpenAIçš„App Store
ä¸€é”®ç”Ÿæˆchat bot å¯¹langchainçš„ç”Ÿæ€ä¹Ÿæ˜¯å¾ˆå¤§çš„æ‰“å‡»
[Image]
