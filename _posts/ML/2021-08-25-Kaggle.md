---
title:  "Kaggle Tips & Tricks"
date:   2021-08-25 21:29:23 +0800
categories: Note ML
toc: true
---

Read two sharings from Kaggle experts.

## 1. My Journey to Becoming a Kaggle Grandmaster [by Darius Baru≈°auskas]

[link](https://www.youtube.com/watch?v=A8oBphPOliM&t=6s)


He was amazing! Achieved master in 1 month, Grandmaster in 12 months and founded his own AI company in 3 years. First dataset and competition he used is SpringLeaf Marketing Response. Reached #4 in 5 days.

Lesson Learned:

1. CPU/RAM management 1# priority
2. Code quality control
3. Work on a single model as long as you can
4. Ignore public LB
5. ML model is a **data generator function approximation**
6. Data tweaking: easy route to better approximation
7. Tree methods are sensitive to noisy features

Look at the data type. Sort. Explore interactions.


## 1. Gold is easy: Kaggle tips and tricks [Chahhou Mohamed]

### General Advice:

1. Positive mind
2. Understand the problem and explore new ideas
3. Don't use kernels
4. In team, only share important insights
5. Keep it simple
6. Don't underestimate NN
7. Transfer ideas


### Feature Engineering

 Don't start by low ranking features(feature that don't divide samples well). They may be correlated. The rankings of the features may be changed by introducing new features and increasing model complexity. In the Titanic example,

 **How to check if a feature is helpful to the model? If the improvement is little? Due to feature or random seed?**

 * First method is Bagging. When adding new features, use bag of models(with different seed) to remove randomness. Three bag of fold CV is better than a single 5 fold CV.

 * Second method is use a bunch of features. Try a full bunch of features at once. Explore the possiblities between features.
 $$ feat_1 = Groupby(feature_{y})[feature_{x}].mean() $$
 $$ feat_2 = feature_{x} - Groupby(feature_{y})[feature_{x}].mean()$$

**How to deal with Outliers(wrong data, missing data)?**


At training phase, you can try three strategies: first is keep the outliers, which provide a baseline. The second is keep the outliers and do winsorisation. The third is remove outliers. Winsorisation is cap the error/target so that the model is not biased by the outliers. To remove the outliers, you should use CV and search for optimal threshold to remove samples from train.

**Additional Tips**

* Blend the model with different subset of features
* When bagging its sometimes good for your model to overfit a little bit
* When you have a lot of features use a very small feature fraction (lgb)
* NN are doing feature engineering implicity
* Use embeddings on discrete numerical features(not just categorical)
* Do NN on tabular data, great in ensembles and performance is almost as good as GBDT
* If you see more improvements from adding a categorical feature to NN than to GBDT, you should try FE with that feature in GBDT
* Randomly replace some values for some features to Null to improve generalization
* When you have Null values in a feature, add a new boolean feature
* Keep the outliers until FE completes or use NN to detect outliers first and correct them in GBDT