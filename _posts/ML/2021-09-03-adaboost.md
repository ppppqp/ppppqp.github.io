---
title:  "AdaBoost"
date:   2021-09-03 21:29:23 +0800
categories: Note ML
---

 > A brief intro on AdaBoost algorithm


AdaBoost(Adaptive Boosting) adjust the order of training with the result from the previous training epoach, so that the weak calssifier can perform better on the errors. 


### Problem Setup

Consider a training set $T = {(x_1, y_1), (x_2, y_2), \cdots, (x_N,y_N)}$ where $x\in\mathbb{R^n}, y\in\{-1,+1\}$

### Decision Stump

Decision stump is a kind of weak classifier that classifies all data based on a single threshold on one coordinate. A decision stump is described by three paramters $\theta_1, \theta_0, k$. $k$ specifies the coordinate(dimension). $\theta_0$ implies the threshold. $\theta_1$ specifies the diretion of classification(above the threshold means positive or negative). 

$$h(x; \theta) = \text{sign}(\theta_1(x_k - \theta_0))$$

The final prediction is a weighted sum of all decision stumps predictions.

$$h_m(x) = \sum_{j=1}^{m} \alpha_i h(x; \theta_j)$$ where $\alpha_i \geq 0$

It can also be considered as a linear classifier with the feature mapping
$$\phi(x) = [h(x; \theta_1), h(x;\theta_2), \cdots, h(x;\theta_m)]^T$$ and
$$ h_m(x) = \alpha^T\phi(x)$$


### Training Process

Adaboost aims to minimize the exponential loss $\sum_{i=1}^n \exp(-y_i h_m(x_i))$

The steps are listed as follows:

1. First epoach: Set all samples as equal-weight. $w_{1,i} = \dfrac{1}{N}$
2. In the $m$ epoach, find the model that yields the smallest error rate $F_m(x)$
   Use the equation for **error rate** $e_m = \underset{i=1}{\overset{N}\sum}w_{m-1,i}I(F_{m}(x_i)\neq y_i)$
   * Error rate is the sum of the weight of all wrongly predicted samples. $w_{m,i}$ is the weight of the $i$ sample in $m$ epoach(generated in $m-1$ epoach)
   * $I$ is the indicator function
3. Update the weight $\alpha_m$ for weak model $F_m$ with $\alpha_m = \dfrac{1}{2}\ln\dfrac{1-e_m}{e_m}$. We add $F_m$ to the strong model $f_m$ with $f_m(x) = f_{m-1}(x) + \alpha_m F_m(x)$
4. Update the weight for the data samples. $w_{m,i} = \dfrac{w_{m-1,i}}{Z_m}\exp{-\alpha_m y_i F_m (x_i)}$
    * $w_{m,i}$ is the previous weight for sample $i$
    * $Z$ is a normalize parameter, $Z_m = \underset{i=1}{\overset{N}{\sum}} w_{m-1,i}\exp{-\alpha_m y_i F_m(x_i)}$.
    * Intepretation: For correctly predicted samples, $-y_{i}F_m(x_i)<0$, which cause the weight to decay. The amount of decay is controlled by $\alpha_m$. All the weight does not depend only on current model,but also the previous models(dependencies on $w_{m-1}$).

5. Iterate over so that the error rate threshold is reached. The final strong model is $\text{sign}[f_M(x)] = \text{sign}[\underset{i=1}{\overset{N}{\sum}}\alpha_i F_i(x)]$


### Regularization

To prevent the issue of overfitting, we introduce a regularization term $v$. The iteration process now becom $f_m(x) = f_{m-1}(x) + v\alpha_m F_m(x)$, $v\in(0,1]$. It controls how many weak classifier is needed to reach the same accuracy.


## Code Implementation
```python
from sklearn.ensemble import AdaBoostClassifier
X =[[1,2], [3,4]]
Y = [1,0]
model = AdaBoostClassifier(random_state=123)
model.fit(X,y)
model.predict([[5,5]])
```