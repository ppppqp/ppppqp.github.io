---
title:  "Support Vector Machine"
date:   2021-09-30 21:29:23 +0800
categories: Note ML MLBasics
---

> An indepth discussion of support vector machine

## What is Support Vector Machine?

In [perceptron](/note/ml/mlbasics/perceptron/) we talked about an easy way to find a linear classifier for data. However, we are not satisfied by merely finding a solution. We are trying to find the best one. That's where SVM(support vector machine) come into play.

Besides the correctness, SVM also take the generalizability into account by finding the linear classifier that maximizes the **margin**.

> Margin: The smallest distance from the data points to the decision boundarty. 



### Hard Margin SVM

Hard margin SVM is a simple form of SVM. Hard margin means the SVM **must correctly classify all data points**. In the set of valid decision boundaries, hard margin SVM choose the one with the largest margin.

The mathematical optimization problem is 

$\underset{\theta, b}{\min}\dfrac{\lVert\theta\rVert^2}{2}$ subject to $y^{(i)}(\theta \cdot x^{(i)} + b) \geq 1)$ for all $i$.

The derivation is quite intuitive: we want to minimize the distance from data point to the hyperplane. The distance is $\dfrac{\theta \cdot x + b}{\lVert\theta\rVert}$. We need $\theta \cdot x + b \geq 1$, so we need to find the minimum $\lVert\theta\rVert$ under this constraint.


### Soft Margin SVM

If the data is not linearly separable, our solution space for hard margin SVM is empty set. Therefore, we need to define another metric, loss, to evaluate how bad the classifier is at predicting. It is a typical example for the ancient bias-variance trade off. In short, we use our loss function to evaluate accuracy of the classifier and use the margin to evaluate the generalizability of the classifier.

The matehmatical optimization problem is

$\underset{\theta, b}{\min}\dfrac{\lVert\theta\rVert^2}{2} + C\sum_{i=1}^{n}\xi_i$ subject to $y^{(i)}(\theta \cdot x^{(i)} + b) \geq 1 - \xi_i)$ for all $i$.

The only difference with hard margin SVM is the introduction of $\xi$(the slack variable). It represent how wrongly the sample is classified by the classifier. By setting $\xi$ to zero, we say it is correctly classified with a safe prediction value(greater than 1).
Situation | Implication
|---|---|
$\xi = 0$ |  correctly classified with a safe prediction value(greater than 1)
$ 0< \xi < 1$ | correctly classified with an unsafe prediction value
$\xi > 1$ | incorrectly classified

Notice there's a coefficient appended before slack variable $\xi$. It is a hyperparameter that balance the weight(i.e, how much you care one vs. another) of the margin and the accuracy. If $C=\inf$, it is essentially a hard margin SVM.


## Lagrange Multiplier

In general, we can transform the original constrained optimization problem into an unconstrained lagrangian formulation.

original problem: $\underset{w}{\min}f(w)$ s.t. $h_i(w)\leq 0$ for all i.
Lagrangian: $ L(w, \alpha) = f(w) + \sum^n_{i=1}\alpha_i h_i(w)$ where $\alpha_i \geq 0$

And what we are looking for is the maximum of the two functions $g_p(w) = \underset{\alpha, \alpha_i\geq 0}{L(w,\alpha)}$. We claim that $g_p(w)$ has the following property:

$$g_{p(w)} =  \left\{
\begin{aligned}
f(w) && \text{if constraints are satisfied}&\\
0 && \text{otherwise}&
\end{aligned}
\right.$$

Proof: 

$$\begin{aligned}
g_p(w) &= \underset{\alpha}{\max}(f(w) + \sum_{i=1}^n \alpha_i h_i (w))\\
&= \underset{\alpha}{\max}(f(w)+\alpha_1h_1(w)+\cdots+\alpha_nh_n(w))
\end{aligned}$$

If constraints are satisfied, then $h(w)\leq 0$. Therefore, given $\alpha\geq 0$, $\alpha$ has to be $0$ to reach the maximum of the function. In this case, $g_p(w) = f(w)$ If constraints are not satisfies, then there exists one $h_i(w)$ such that $h_i(w) \geq 0$. Then, the function reaches the maximum($\infty$) when the corresponding $\alpha_i \rightarrow \infty$.

We have successfully transformed the origin constrained optimization problem by introducing a lagrange multiplier. Our problem now is
$$\min_\theta \max_{\alpha} L(\theta, \alpha)$$


## Dual Formulation

Solving the above optimization problem is kind of difficult. A natural language interpretation is: "find the $\theta$ that minimizes the formula, which is given by $L(\alpha, \theta)$ where $\alpha$ takes the non-negative values that maximizes the L." To solve it we need to represent $\alpha$ in the form of $\theta$ by solving given $\theta$, what is the optimal $\alpha$(think of fixing a coordinate while analyzing the other). However, we also have the constraint of $\alpha \geq 0$. 

We choose to inspect the dual formulation. The dual formulation (contrast with primal formulation) is the following:

Type | Formulation
|---|---|
Primal | $$\min_w \max_{\alpha, \alpha\geq 0}L(w,\alpha) $$
Dual | $$\max_{\alpha, \alpha\geq 0}\min_w L(w,\alpha) $$


First, we need to make sure that the dual formulation is feasible. The difference between the solution of two forms is called **duality gap**. Generally the dual gives a lower bound on the solution of the primal. However, under certain conditions(quadratic convex objective, constraint functions affine, primal/dual feasible), the duality gap is 0.

* Our objective function is $\min_\theta \dfrac{\lVert \theta \rVert^2}{2}$. 
* Our constraint is $y^{(i)}(\theta \cdot x^{(i)}) \geq 1$ for every $i$. 
* The lagrange formulation is $L(\theta, \alpha) = \dfrac{\lVert \theta \rVert^2}{2} + \sum_{i=1}^{n}\alpha_i(1-y^{(i)}(\theta \cdot x^{(i)}))$ with $\alpha_i \geq 0$
* In dual formulation: $\max_{\alpha, \alpha\geq 0}\min_\theta L(\theta,\alpha) $

We can further simplify the dual formulation by substituting $\theta$ with $\alpha$. Specifically, we find the optimal $\theta$ given $\alpha$ by taking the gradient with respect to $\theta$:

$$\begin{aligned}
\theta^{*} - \sum_{i=1}^{n}\alpha_iy^{(i)}x^{(i)} = 0\\
\theta^{*} = \sum_{i=1}^{n}\alpha_iy^{(i)}x^{(i)}
\end{aligned}$$

Plugin the $\theta^{*}$ value, we can have the formulation with only $\alpha$ terms:

$$\max_{\alpha, \alpha_i \geq 0} \sum_{i=1}^{n}\alpha_i -\dfrac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_j y^{(i)}y^{(j)}x^{(i)}\cdot x^{(j)}$$

Then we can use gradient descent to solve this differentiable problem. That is, we take the partial derivative of each $\alpha_i$ and optimize. The final optimal $\theta^{*}$ value is fully described by $\alpha$. Specifically, if $\alpha_{i} > 0$, it means $y^{(i)}\theta^*\cdot x^{(i)} = 1$. The data point lies on the margin exactly, called **support vector**. if $\alpha_{i} = 0$, it means $y^{(i)}\theta^*\cdot x^{(i)} > 1$. The data point lies outside the margin, called **non-support vector**.


### SVM with Offset dual form

Before we only considered hyperplane that cross the origin. What if we want learn a hyperplane with offset? 

* Now the objective function is $\min_\theta \dfrac{\lVert \theta \rVert^2}{2}$. 
* Our constraint is $y^{(i)}(\theta \cdot x^{(i)} + b) \geq 1$ for every $i$. 
* Now the language formulation is $L(\theta, \alpha) = \dfrac{\lVert \theta \rVert^2}{2} + \sum_{i=1}^{n}\alpha_i(1-y^{(i)}(\theta \cdot x^{(i)} + b))$ with $\alpha_i \geq 0$

* The dual formulation is $\max_{\alpha, \alpha\geq 0}\min_{\theta, b} L(\theta,b,\alpha) $


 We substitute both  $\theta$ and $b$ using the first derivative.

 $$\begin{aligned}
\theta^{*} = \sum_{i=1}^{n}\alpha_iy^{(i)}x^{(i)}\\
\nabla_{b} L(\theta, \alpha)\lvert _{b = b^*} = 0 \\
\sum_{i=0}^n \alpha_i y_i = 0
\end{aligned}$$

Therefore, the effect of offset is a new constraint: $\sum_{i=0}^n \alpha_i y_i = 0$


### Soft margin SVM dual form

* Now the objective function is $\min_\theta \dfrac{\lVert \theta \rVert^2}{2} + C\sum \xi_i$. 
* Our constraint is $y^{(i)}(\theta \cdot x^{(i)} + b) \geq 1 - \xi_i$ for every $i$ and $\xi_i \geq 0$
* Language formulation is 
  * $L(\theta, b, \xi, \alpha, \beta) = \dfrac{\lVert \theta \rVert^2}{2} + C\sum \xi_i + \sum_{i=1}^{n}\alpha_i(1-y^{(i)}(\theta \cdot x^{(i)} + b) - \xi_i) - \sum_{i=1}^{n}\beta_i \xi_i$
  * with $\alpha_i, \beta_i \geq 0$

* The dual formulation is $\max_{\alpha, \beta, \alpha\geq 0, \beta\geq 0}\min_{\theta, b, \xi} L(\theta,b,\xi,\alpha, \beta) $


Besides the previous constraints, we need to consider $\xi$.

$$\begin{aligned}
\nabla_{\xi} L(\theta, \alpha)\lvert _{\xi = \xi^*} = 0 \\
C - \alpha_i - \beta_i = 0\\
\alpha_i = C - \beta_i
\end{aligned}$$

Therefore, the overall constraints are:


 $$\left\{\begin{aligned}
    \sum_{i=0}^{n} \alpha_i y_i=0\\
    \alpha_i, \beta_i \geq 0\\
    \alpha_i + \beta_i = C
\end{aligned}\right.$$

It shows that soft margin svm is an optimization problem with hinge loss as objective function and L2-norm regularizer.

### Feature Mapping

In some case, the data is not linearly separable in the original space, but may be easily separated in another space. 


{% include figure image_path="/assets/images/ML/feature_mapping.png" alt="this is a placeholder image" caption="not linearly separable" %}

In this case, we can map our features to a higher dimensional space and find a hyperplane at that space. For example, we can map all data points $(x_i, y_i)$ to a new space $(\phi(x_i), y_i)$ and explore the relation at that space.


$$\max_{\alpha, \alpha_i \geq 0} \sum_{i=1}^{n}\alpha_i -\dfrac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^{n}\alpha_i\alpha_j y^{(i)}y^{(j)}\phi(x^{(i)})\cdot \phi(x^{(j)})$$


However, interms of SVM, feature mapping is not very efficient because we need to do high dimensional computation on $\phi(x_i)\cdot \phi(x_j)$. We can reduce the effect using kernel trick. Kernel trick is basically replacing $\phi(x_i)\cdot \phi(x_j)$ with a function $K$ that directly take in $x_i, x_j$, that is, 
$$ \phi(x_i)\cdot \phi(x_j) = K(x_i, x_j)$$

Recall we use $h(x) = \text{sign}(\theta \cdot x)$ to make classification. Plug in the value of $\theta = \sum_{i=1}^n \alpha_i y^{(i)}x^{(i)}$, we get $h(x) = \text{sign}(\sum_{i=1}^{n} \alpha_i y^{(i)}x^{(i)}\cdot x)$. If we have feature mapping, it is  $h(x) = \text{sign}(\sum_{i=1}^{n} \alpha_i y^{(i)}\phi(x^{(i)})\cdot \phi(x))$. Using kernel subsitute, we have $h(x) = \text{sign}(\sum_{i=1}^{n} \alpha_i y^{(i)}K(x^{(i)}\cdot x))$, which is easier to compute.

### Kernal Algebra


We can have arbitrary feature mapping, but not all kernel functions are valid(i.e corresponds to a feature mapping)
We have the following rules for kernel algebra. Let $K_1, K_2$ be valid kernel, then the followings are valid kernels.


* $K(x, z) = K_1(x,z)+ K_2(x,z)$
* $K(x,z) = \alpha K_1(x,z)$ where $\alpha > 0$
* $K(x,z) = K_1(x,z)K_2(x,z)$

### Mercer's Theorem

A function K:$R^d \times R^d \rightarrow R$ is a valid kernel iff for any $x^{(1)}, \cdots, x^{(n)}$ with $x^{(i)}\in \mathbb(R)^{d} $and finite $n$, the $n\times n$ matrix $G$ with $G_{ij} = K(x^{(i)},x^{(j)})$ is positive-semidefinite. That is, $G$ is symmetric and for any $z\in \mathbb{R}^n$, $z^T G z \geq 0$


### Kernel Feature Mapping Conversion

How can we know the other if we know one of the kernel function and feature mapping function?

If we know the feature mapping function $\phi$, we can find the $K$ through the following steps.
1. Clarify the each terms of $\phi(x)$. Suppose $$\phi([x_1, x_2]) = [\phi_1(x_1, x_2), \phi_2(x_1, x_2),\phi_3(x_1, x_2)]$$
2. Write $\phi(x)\cdot\phi(z)$. Note that $\phi(x)\cdot \phi(z) = K(x\cdot z)$.
3. Express $\phi(x)\cdot\phi(z)$ in a function of $x\cdot z$. This function is the kernel function. Remember kernel function is a $\mathbb{R} \rightarrow \mathbb{R}$ function. 

For example, if we have a feature mapping $\phi([x_1, x_2]) =[2x_1^3, 2x_2^3, 2\sqrt3x_1^2x_2, 2\sqrt3 x_2^2x_1]$, we derive the kernel function
$$\begin{aligned}
&\phi([x_1, x_2])\cdot\phi([z_1, z_2])\\
=&4x_1^3z_1^3+4x_2^3z_2^3+12x_1^2x_2z_1^2z_2+12x_2^2x_1z_2^2z_1 \\
=&4(x_1^3z_1^3 + x_2^3z_2^3 + 3x_1^2x_2z_1^2z_2 + 3x_2^2x_1z_2^2z_1)\\
=& 4(x_1z_1 + x_2z_2)^3\\
=& 4([x_1, x_2]\cdot [z_1, z_2]])\\
=& K([x_1, x_2], [z_1, z_2])
\end{aligned}$$

If we know the kernel function, we can find the $\phi$ through the following steps:
1. Compute $K(x,z)$ and spread the terms.
2. For each term, symmetrically separate $x$ terms and $z$ terms.
3. View as a dot product of two similar vectors.
4. The vector is what we need.

### How can we use Kernel exactly?


Consider we use the dual form to solve the following data points with a kernel $K(x,z) = 4(x\cdot z)^3$

{% include figure image_path="/assets/images/ML/feature_mapping.png" alt="this is a placeholder image" caption="not linearly separable" %}

Suppose we want to find

#### The offset $b$

We observe that $x^{(3)}$ and $x^{(4)}$ lies on the margin. Therefore, we know the distance from the two points to the decision boundary is 1. We choose $x^{(3)}$ to do the computation:

$$\begin{aligned}
 \theta \cdot \phi(x^{(3)}) + b &= 1\\
 b  &= 1 - \theta \cdot \phi(x^{(3)})\\
    &= 1 - \sum_{i=1}^{5}\alpha_i y^{(i)}\phi(x^{(i)}) \cdot \phi(x^{(3)})\\
    &= 1 - \alpha_3 y^{(3)} \phi(x^{(3)}) \cdot \phi(x^{(3)}) - \alpha_4 y^{(3)} \phi(x^{(4)}) \cdot \phi(x^{(3)})\\
    &= 1 - 0.00128 \cdot  K(x^{(3)}, x^{(3)}) + 0.00128 \cdot K(x^{(4)}, x^{(3)})\\
    & = 1 - 0.00128 \cdot 4 \cdot 1^3 + 0.00128 \cdot 4 \cdot 4^3\\
    &=1.32
 \end{aligned}$$


 #### Make a prediction

 Suppose we want to make a prediction on $x= [-2,1]^T$. Let's see how we can apply the kernel.

$$\begin{aligned}
&\theta \cdot [-2,1]+b\\
=& \sum_{i=1}^{5}\alpha_i y^{(i)}\phi(x^{(i)})[-2,1] + b\\
=& \alpha_3 y^{(3)} \phi(x^{(3)}) \cdot \phi([-2,1]) - \alpha_4 y^{(3)} \phi(x^{(4)}) \cdot \phi([-2,1]) + 1.32\\
=& 0.00128 \cdot K([1,1]\cdot [-2,1]) - 0.00128 \cdot K([2,2]\cdot [-2,1]) \\
=&0.00128 \cdot 4 \cdot (-1)^3 - 0.00128 \cdot 4 \cdot (-2)^3 + 1.32\\
\gt &0
\end{aligned}
$$

Therefore it is classified as positive sample.