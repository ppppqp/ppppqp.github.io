---
title:  "GBDT"
date:   2021-09-07 21:29:23 +0800
categories: Note ML
mermaid: true
---

## Main Idea

The main idea of GBDT(Gradient Boosting Decision Tree) is that we fit the residuals based on the weak models to get strong models. It is different from AdaBoost in that AdaBoost adjust the weight of models while GBDT fit the next model with the residual of the previous one.

Suppose we have a dataset:

Customer|Age|Income|credit
|---|---|---|---|
A | 24 | 10000 | 8000
B | 28 | 20000 | 30000
C | 32 | 15000 | 25000
D | 30 | 25000 | 40000

We can build a regression decision tree based on the data.


<div class="mermaid">
graph TD
    A[income < 20000] -- yes --> B[age < 25]
    A -- No --> C[credit is 35000]
    B -- Yes --> D[credit is 10000]
    B -- No --> E[credit is 20000]
</div>

However, the MSE is not good enough.  


$$\sqrt{2000^2+5000^2+5000^2+5000^2}$$

Therefore, we boost this model with another one based on the residual of it.


Now the dataset can be.

Customer|Age|Income|residual
|---|---|---|---|
A | 24 | 10000 | -2000
B | 28 | 20000 | 5000
C | 32 | 15000 | -5000
D | 30 | 25000 | 5000


<div class="mermaid">
graph TD
    A[age < 30] -- yes --> B[income < 15000]
    A -- No --> C[credit is 5000]
    B -- Yes --> D[credit is -3000]
    B -- No --> E[credit is -5000]
</div>

Now we combine the result of two trees, which gives us the prediction value:
* A: 7000
* B: 5000
* C: 5000
* D: 5000

which gives a pretty good prediction.


## Mathematrical Representation

The model we maintain is $f_m(x) = f_{m-1}(x)+T_m(x)$

The steps to get $T_m(x)$ is as follows.

* Step1: initialize $f_0(x)=0$
* Step2: at $m$ iteration, calculate the residual over the strong model $r_{m_i} = y_i - f_{m-1}(x) + T_m(x)$
* Step3: iterate the above steps

## Code Implementation

```python
from sklearn.ensemble import GradientBoostingClassifier

X = [[1,2],[3,4]]
y = [0,1]

mdoel = GradientBoostingClassifier(ramdom_state = 1)
model.fit(X,y)
print(model.predict([[5,5]])

```
