---
title:  "ROC and KS curve"
date:   2021-09-01 21:28:23 +0800
categories: Note ML
---


## Background

### ROC curve
When we are doing logistic regression or more broadly, classification problems, we care more about accuracy, i.e $$\dfrac{true\ predictions}{all\ predictions}$$
Instead, we look into the metrics of true positive rate(TPR) and false positive rate(FPR), defined below:
$$TPR = \dfrac{positives\ that\ predict\ to\ be\ positive}{positives}$$
$$FPR = \dfrac{negatives\ that\ predict\ to\ be\ positive}{negatives}$$

TPR also has name "Recall". A successful model should have TPR as high as possible and FPR as low as positive. However, these two metrics seems to be highly correlated. If you raise the threshold, that is, you make it more difficult to predict positive, you will end up with lower FPR but also lower TPR, and vice versa. Therefore, to compare models, we plot a ROC curve for each of them. Note that **one model, one ROC curve**. Change in threshold doesn't make it another model.

ROC curve is the relationship between FPR and TPR under different threshold. We can think of it as

$$ FPR = f(threshold)\\TPR = g(threshold) $$

and in this way, they are both parametrically represented by a common variable. Therefore, we can plot them in a 2-D curve. **AUC** means the area under the curve. It is obvious that the greater the area is, the better the model is.

{% include figure image_path="/assets/images/ML/ROC.png" alt="this is a placeholder image" caption="ROC curve" %}

### KS curve

KS curve is fundamentally the same as ROC curve. The differences are that KS curve use threshold as x axis and the difference between TPR and FPR as the y axis.

{% include figure image_path="/assets/images/ML/KS.png" alt="this is a placeholder image" caption="KS curve" %}


Notice that:

1. When threshold is 0%, both TPR and FPR is 0 because no sample is predicted as positive.
2. TPR concave up and FPR concave down. This is because when threshold just increases from 0%, there are more FP rather than TP, so the derivative of FTP is greater than TPR.
3. The metrics corresponding to KS curve is its peak value.
    * peak value < 0.2: weak capability
    * peak value > 0.3: great capability
## Code Implementation:


### 1. Confusion matrix

```python
from sklearn.metrics import confusion_matrix
m = confusion_matrix(y_test, y_pred)

```

### 2. Accuracy, TPR, FPR

```python
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))
```
### 3. ROC curve

```python
from sklearn.metric import roc_curve
fpr, tpr, thres = roc_curve(y_test, y_pred_proba[:,1])

from matplotlib.pyplot as plt
plt.plot(fpr, tpr)
```

### 4. AUC score

```python
from sklearn.metrics import roc_auc_score
score = roc_auc_score(y_test, _pred_proba[:,1])
```

### 5. KS curve

```python
plt.plot(threshold, tpr)
plt.plot(threshold, fpr)
plt.plot(threshold, tpr-fpr)

plt.gca().invert_axis()
# invert axis to get a curve from 1 to 0

max(trp-fpr) # get KS value

```