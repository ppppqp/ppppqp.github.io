---
title:  "Some BERT Conversation"
date:   2021-09-03 21:28:23 +0800
categories: Note ML
---

> 和博士大佬的关于bert的小争执

起因是博士大佬认为bert的出现其实使得NLP的进展倒退了，而我非常困惑，于是就跑去对线了...

### 先说下我自己的理解
我对bert其实也不算很了解，看过论文，只是懂了个大概（核心思想，比如深层双向encoder，Masked LM，position embedding，以后可以专门写一个记录一下自己的理解）。主要对bert的地位还是从[这片文章](https://zhuanlan.zhihu.com/p/395115779)中了解的。这篇文章其实算是[论文](https://arxiv.org/pdf/2107.13586.pdf)的summary，我这边也可以再给一个summary...毕竟这篇论文长达47页。

这片论文主要讲了NLP发展的一些规律。总结下来，NLP的发展经历了四个范式：

* P1. 非神经网络时代的完全监督学习 （Fully Supervised Learning, Non-Neural Network）
* P2. 基于神经网络的完全监督学习 (Fully Supervised Learning, Neural Network)
* P3. 预训练，精调范式 (Pre-train, Fine-tune)
* P4. 预训练，提示，预测范式（Pre-train, Prompt, Predict)

总结了一下一些演变的规律，也就是为什么后面的范式会取代前面的。我认为其实这也是机器学习越来越接近人类学习的规律。

#### 规律1:  每个范式都会设计繁琐的，需要人来参与的工程
可以罗列出所有的人工所需要的环节
* P1：特征工程，特征模版定义环节
* P2：结构工程，虽然解放了手动配置特征模版所需要的人力，但是需要人去设计合适的网络结构。
* P3：引入额外的目标函数到预训练模型上，以便适配下游任务。需要网络结构的挖掘，但相比较而言，不是范式的主旋律。原因在于：
  * 预训练过程本身费时，过度的结构偏置开销过大。
  * 精调的过程中，不需要注意神经网络之间的差异
* P4：不改动预训练模型，而是通过合适的prompt的利用将下游任务建模的方式重新定义。

作者也给出了从这个规律中能学到的经验：
> 考虑新的范式的时候，需要
> 1. 考虑新增的人力代价
> 2. 当我们某个时候有些神奇的想法，与现在主流的解决思路不相同，又需要一些细碎的操作，那这个时候可能就在接近一种新的范式。

其实也就是可执行性吧，很多ML的训练任务都会因为数据集/人工标注/调参 的人力/时间问题受限而不能持续。这也侧面反映了新范式出现的意义：（暂时）打破大公司的资源碾压，实现AI平等。

#### 规律2: 新范式带来的收益可以让我们暂时“忽略”那些额外需要的人力代价。

接上条，探讨了范式之间迭代的规律。

具体作者给出的例证如下：
* P1->P2：从炼丹向陶艺发展（模型的形状是关键，而不是ingredient）
* P2->P3：从模型组合到损失函数选择 （不再需要关注模型形状，而关注下游任务）
* P3->P4：激活其他研究场景（小样本学习）

还有一个有趣的点在于，收益的定义不一定只在某一研究场景下资源的有效利用，也可以是研究场景的统一（个人认为这个更有意义，因为更接近人工智能的本质）。

> 关于“收益”的定义并不唯一，它不只是被定义为某个任务性能的提升，还可以是新的研究场景。比如，无论是神经网络早期在NLP的应用，或者是Seq2Seq 早期在翻译上的应用，都没有在性能上讨得便宜，可是这种颠覆性的想法给了我们太多可以想象的空间（比如既然翻译可以用Seq2Seq，那么其他任务是不是也可以这样呢？那么NLP任务解决框架就可以被统一了吗？）

最后，作者介绍了一下prompt learning，与本文主题无关，可以后面另开篇讲。

总之，作者对bert和NLP的发展是持乐观态度的。


### 博士大佬的看法

我厚着脸皮去博士大佬的朋友圈下评论一番后，大佬很nice的回复了我。原文找不到了，我只能凭记忆东拼西凑一下。


> Bert作为一个万剑归宗的Pretrained Model，什么任务似乎都变得方便了，但其实会摒弃很多原来的研究(比如cognitive方面)。其实NLP没有那么简单，文字也没有那么简单的可以转化为vector。一个pretrained model作为一个黑盒可能隐藏这许多歧视和偏见。

其实是不错的。作为工业界，肯定对bert是非常欢迎的，因为它大大减少了工作量，能适应很多下游任务。我觉得博士大佬作为学术界代表，其实更加关注作为一个AI的可解释性和更深层次的驱动力（比如cognitive上的一些研究）这些对于AI的上限是非常关键的，因为我们现在只能让AI用建模的方式去模拟一个语言空间，AI在中间能作出的inference也是局限在这个空间的样本里。如果要让AI能自己学习成长，一定是需要对语言更丰富的表现形式。简单地扩张向量的维数，确实不一定是最优选择。

