---
title:  "[NLP] ELMo"
date:   2021-09-30 21:29:23 +0800
categories: Note ML
---

> A brief intro to ELMO paper and the context.

[Ref](https://www.mihaileric.com/posts/deep-contextualized-word-representations-elmo/)

In this post I will talk a little bit about ELMo(Embeddings from Language Models). 

## Context

Before we dive into what is ELMo, I would like you to read the [word2vec](/note/ml/NLPmodels/). word2vec is developed in 2014, and ELMo is developed in 2018. However, they are all groundbreaking ideas that pushing forward the NLP research. As I talked in post, word2vec basically proposed two architecutres(CBOW and skip-gram) to train the word vector. Every word is represented by a vector after the model is properly trained. The result for word2vec is very inspiring: we are finally able to map each word in a space where we can do vector manipulations. Some interesing interactions between word vectors, like Vector("King") - Vector("Man") + Vector("Woman") = Vector("Queen") is learnt by the model, indicating its ability to measure both the sematic and syntatic simlarity.

## Problem
Despite the great success of word2vec, ELMo finds an important flaw in pretrained word vector: it is static and context-unaware. In reality, what a word mean actually depends on the context. For example, in sentences "I wil buy you a present" and "Live in the present, not the past", the word "present" has different meaning based on the context. Word2vec only gives the same vector for both "present". It is likely that in high dimensions both meaning is captured. However, we don't know if that's true. 

## ðŸ’¡ ELMo's Idea
ELMo propose that we should not have a static word embedding. By contrast, we should generate the word embedding dynamically based on the context. Specifically, we should train a language model such that when given a sentence, it is able to produce the specific embedding for a word in exactly that sentence. This is also where the ELMo's name: Embeddings from Language Models comes from.

## ELMo's Approach

### BiLM
ElMo's architecture is basically a two layer bi-directional LSTM.

{% include figure image_path="/assets/images/ML/elmo_biLM.png" alt="this is a placeholder image" caption="ELMo biLM" %}

As shown in the graph, the word is fed into the first biLSTM, runs forward and backward, and then fed into the second one. There's a misleading part of this graph: the original version of ELMo runs two layers separately then concatenate rather than feeding the output of one into the other. However, I think it makes sense to feed the output of one layer to the other because the two layers actually have different levels of representation.

Residual connections are also added between two layers in order to reduce complexity as well as put different weights on each layer. With residual connections, the word can be directly fed into the second layer.

{% include figure image_path="/assets/images/ML/elmo_flow.png" alt="this is a placeholder image" caption="ELMo biLM Residual" %}

### Character Embedding

So how are word fed into the LSTM? Traditionally we convert a word into a fixed length vector before fed into the recurrent unit, either in one-hot or distributed form. It makes no sense in ELMo because we are actually generating a word embedding.

ELMo uses a different approach called "character embedding." It brings multiple benefits as stated in the reference. First, it encode morphological similarities, like "beautiful" and "beautifully." Second, it resolves the Out of Vocabulary problem. Thrid, the CNN layer is able to see n-gram features in different scales.

{% include figure image_path="/assets/images/ML/elmo_character.png" alt="this is a placeholder image" caption="Character Embeddings" %}

### How to Use ELMo

Once ELMo is trained, we can fit it into different down stream tasks by fine tuning. We have three representation for a word: character embedding, first layer of LSTM and second layer of LSTM. We can use the linear combination of them as the final output, and by adjusting the weights of each represenation, we can fit different down stream tasks.

{% include figure image_path="/assets/images/ML/elmo_final.png" alt="this is a placeholder image" caption="Fine Tuning" %}
